{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with NOAA dataset\n",
    "\n",
    "The journal article describing GHCN-Daily is:\n",
    "\n",
    "Menne, M.J., I. Durre, R.S. Vose, B.E. Gleason, and T.G. Houston, 2012: An overview of the Global Historical Climatology Network-Daily Database. Journal of Atmospheric and Oceanic Technology, 29, 897-910, doi:10.1175/JTECH-D-11-00103.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple output per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = '/media/data-nvme/dev/datasets/WorldBank/'\n",
    "noaa_csv_path = DATASET_FOLDER + 'noaa/ASN*.csv'\n",
    "SPARK_MASTER = 'spark://192.168.0.9:7077'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14141736\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('pi')\n",
    "conf = conf.setMaster(SPARK_MASTER)\n",
    "sc = SparkContext(conf=conf)\n",
    "import random\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627989"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noaa_csv_path = '/media/data-nvme/dev/datasets/WorldBank//noaa/ASN00060066.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/ASN*.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/*.csv'\n",
    "noaa_csv_path = DATASET_FOLDER + '/small_dataset/*.csv'\n",
    "def count_lines():\n",
    "    # configuration\n",
    "    APP_NAME = 'count NOAA all lines'\n",
    "    conf = SparkConf().setAppName(APP_NAME)\n",
    "    conf = conf.setMaster(SPARK_MASTER)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # core part of the script\n",
    "    files = sc.textFile(noaa_csv_path)\n",
    "    total = files.count()\n",
    "#     lineLength = lines.map(lambda s: len(s))\n",
    "#     lineLength.persist()\n",
    "#     totalLength = lineLength.reduce(lambda a,b:a+b)\n",
    "#     # output results\n",
    "#     print(totalLength)\n",
    "    sc.stop()\n",
    "    return total\n",
    "\n",
    "total = count_lines()\n",
    "#print(f'{totalLength:%.2f}')\n",
    "total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627,989\n"
     ]
    }
   ],
   "source": [
    "print(f'{total:,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of lines : 1 076 166 433\n",
    "\n",
    "# Filters\n",
    "\n",
    "https://docs.opendata.aws/noaa-ghcn-pds/readme.html\n",
    "\n",
    "https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt\n",
    "\n",
    "Keep only \n",
    "\n",
    "ELEMENT Summary\n",
    "\n",
    "The five core elements are:\n",
    "\n",
    "    PRCP = Precipitation (tenths of mm)\n",
    "    SNOW = Snowfall (mm)\n",
    "    SNWD = Snow depth (mm)\n",
    "    TMAX = Maximum temperature (tenths of degrees C)\n",
    "    TMIN = Minimum temperature (tenths of degrees C)\n",
    "\n",
    "    Variable   Columns   Type\n",
    "    ------------------------------\n",
    "    ID            1-11   Character\n",
    "    YEAR         12-15   Integer\n",
    "    MONTH        16-17   Integer\n",
    "    ELEMENT      18-21   Character\n",
    "    VALUE1       22-26   Integer\n",
    "    MFLAG1       27-27   Character\n",
    "    QFLAG1       28-28   Character\n",
    "\t\t   \n",
    "VALUE1     is the value on the first day of the month (missing = -9999).\n",
    "\n",
    "      \"PRCP_ATTRIBUTES\" = a,M,Q,S where:\n",
    "           a = DaysMissing (Numeric value): The number of days (from 1 to 5) missing or flagged is provided   \n",
    "           M = GHCN-Daily Dataset Measurement Flag (see Section 1.3.a.ii for more details) \n",
    "           Q = GHCN-Daily Dataset Quality Flag (see Section 1.3.a.iii for more details)\n",
    "           S = GHCN-Daily Dataset Source Code (see Section 1.3.a.iv for more details)  \n",
    "\n",
    "MFLAG1     is the measurement flag for the first day of the month.  There are\n",
    "           ten possible values:\n",
    "\n",
    "           Blank = no measurement information applicable\n",
    "           B     = precipitation total formed from two 12-hour totals\n",
    "           D     = precipitation total formed from four six-hour totals\n",
    "\t   H     = represents highest or lowest hourly temperature (TMAX or TMIN) \n",
    "\t           or the average of hourly values (TAVG)\n",
    "\t   K     = converted from knots \n",
    "\t   L     = temperature appears to be lagged with respect to reported\n",
    "\t           hour of observation \n",
    "           O     = converted from oktas \n",
    "\t   P     = identified as \"missing presumed zero\" in DSI 3200 and 3206\n",
    "           T     = trace of precipitation, snowfall, or snow depth\n",
    "\t   W     = converted from 16-point WBAN code (for wind direction)\n",
    "\n",
    "QFLAG1     is the quality flag for the first day of the month.  There are \n",
    "           fourteen possible values:\n",
    "\n",
    "           Blank = did not fail any quality assurance check\n",
    "           D     = failed duplicate check\n",
    "           G     = failed gap check\n",
    "           I     = failed internal consistency check\n",
    "           K     = failed streak/frequent-value check\n",
    "\t   L     = failed check on length of multiday period \n",
    "           M     = failed megaconsistency check\n",
    "           N     = failed naught check\n",
    "           O     = failed climatological outlier check\n",
    "           R     = failed lagged range check\n",
    "           S     = failed spatial consistency check\n",
    "           T     = failed temporal consistency check\n",
    "           W     = temperature too warm for snow\n",
    "           X     = failed bounds check\n",
    "\t   Z     = flagged as a result of an official Datzilla \n",
    "\t           investigation\n",
    "    \n",
    "- WESF = Water equivalent of snowfall (tenths of mm)\n",
    "\n",
    "WV** = Weather in the Vicinity where ** has one of the following values:\n",
    "\n",
    "    01 = Fog, ice fog, or freezing fog (may include heavy fog)\n",
    "    03 = Thunder\n",
    "    07 = Ash, dust, sand, or other blowing obstruction\n",
    "    18 = Snow or ice crystals\n",
    "    20 = Rain or snow shower\n",
    "\n",
    "WMO ID is the World Meteorological Organization (WMO) number for the station. If the station has no WMO number (or one has not yet been matched to this station), then the field is blank.\n",
    "\n",
    "     \n",
    "\n",
    "HCN/CRN FLAG = flag that indicates whether the station is part of the U.S. Historical Climatology Network (HCN). There are three possible values:\n",
    "\n",
    "    Blank = Not a member of the U.S. Historical Climatology or U.S. Climate Reference Networks\n",
    "    HCN = U.S. Historical Climatology Network station\n",
    "    CRN = U.S. Climate Reference Network or U.S. Regional Climate Network Station\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: ligne 0: fin de fichier (EOF) prématurée lors de la recherche du « ' » correspondant\n",
      "/bin/bash: -c: ligne 1: erreur de syntaxe : fin de fichier prématurée\n"
     ]
    }
   ],
   "source": [
    "!head /media/data-nvme/dev/datasets/WorldBank//noaa/ASN00060066.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $DATASET_FOLDER/noaa/ASN00060066.csv\n",
    "!wc -l $DATASET_FOLDER/noaa/ASN00060066.csv\n",
    "!grep 2016 $DATASET_FOLDER/noaa/AE000041196.csv | head -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /media/data-nvme/dev/datasets/WorldBank/year_2016-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter on Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /media/data-nvme/dev/datasets/WorldBank//small_dataset/*.csv...\n",
      "Saving to /media/data-nvme/dev/datasets/WorldBank/year_2016-01\n",
      "CPU times: user 18.1 ms, sys: 13.5 ms, total: 31.7 ms\n",
      "Wall time: 5.29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#noaa_csv_path = '/media/data-nvme/dev/datasets/WorldBank//noaa/ASN00060066.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/AE*.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/*.csv'\n",
    "noaa_csv_path = DATASET_FOLDER + '/small_dataset/*.csv'\n",
    "\n",
    "def filter_year():\n",
    "    total = 0\n",
    "    # configuration\n",
    "    APP_NAME = 'Filter on Year'\n",
    "    conf = SparkConf().setAppName(APP_NAME)\n",
    "    conf = conf.setMaster(SPARK_MASTER)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    try:\n",
    "        # core part of the script\n",
    "        print(f'Processing {noaa_csv_path}...')\n",
    "        files_rdd = sc.textFile(noaa_csv_path)\n",
    "        year_2016  = files_rdd.filter(lambda s : \"\\\"2016-01-01\" in s)\n",
    "        output = DATASET_FOLDER + 'year_2016-01'\n",
    "        print(f'Saving to {output}')\n",
    "        year_2016.saveAsTextFile(output)\n",
    "        total = year_2016.count()\n",
    "    except Exception as inst:\n",
    "        print('ERROR')\n",
    "        print(type(inst))    # the exception instance\n",
    "        print(inst.args)     # arguments stored in .args\n",
    "        print(inst)\n",
    "        raise\n",
    "    finally:\n",
    "        sc.stop()\n",
    "        return total\n",
    "\n",
    "total = filter_year()\n",
    "#print(f'{totalLength:%.2f}')\n",
    "total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter on Rain - Python IN version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '\"USR0000AFRA\",\"2016-01-25\",\"35.8456\",\"-113.055\",\"2063.5\",\"FRAZIER WELLS ARIZONA, AZ US\",\"   39\",\"H,,U\",\"  -39\",\"H,,U\",\"  -10\",\",,U\"'\n",
    "s[1:12]\n",
    "s[14:]\n",
    "s = 'ACW00011604  17.1167  -61.7833 TMAX 1949 1949'\n",
    "s[0:11]\n",
    "s[36:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/*.csv'\n",
    "noaa_csv_path = DATASET_FOLDER + 'year_2016-01.csv'\n",
    "inventory_path = DATASET_FOLDER + 'ghcnd-inventory.txt'\n",
    "def extract_rain():\n",
    "    total = 0\n",
    "    # configuration\n",
    "    APP_NAME = 'Filter on Rain'\n",
    "    conf = SparkConf().setAppName(APP_NAME)\n",
    "    conf = conf.setMaster(SPARK_MASTER)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    try:\n",
    "        # Load inventory\n",
    "        inventory_rdd = sc.textFile(inventory_path)\n",
    "        rain_inventory_rdd = inventory_rdd.filter(lambda s : \"PRCP\" in s)\n",
    "        # Now we have a list of all files containings precipitation data\n",
    "        # Keep only the first column\n",
    "        rain_inventory_rdd = rain_inventory_rdd.filter(lambda s: s[0:11])\n",
    "        rain_inventory = rain_inventory_rdd.collect()\n",
    "        # Load the stations data points\n",
    "        print(f'Processing {noaa_csv_path}...')\n",
    "        files_rdd = sc.textFile(noaa_csv_path+'/*')\n",
    "        # Keep only precipitation data\n",
    "        rain_rdd  = files_rdd.filter(lambda s : s[1:12] in rain_inventory)\n",
    "        # Save precipitation data\n",
    "        rain_rdd.saveAsTextFile(DATASET_FOLDER + 'year_2016-01_rain')\n",
    "        total = rain_rdd.count()\n",
    "    except Exception as inst:\n",
    "        print('ERROR')\n",
    "        print(type(inst))    # the exception instance\n",
    "        print(inst.args)     # arguments stored in .args\n",
    "        print(inst)\n",
    "        raise\n",
    "    finally:\n",
    "        sc.stop()\n",
    "        return total\n",
    "\n",
    "total = extract_rain()\n",
    "#print(f'{totalLength:%.2f}')\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter on Rain - Join version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /media/data-nvme/dev/datasets/WorldBank/year_2016-01_rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /media/data-nvme/dev/datasets/WorldBank/year_2016-01...\n",
      "Saving to /media/data-nvme/dev/datasets/WorldBank/year_2016-01_rain\n",
      "CPU times: user 20.4 ms, sys: 5.09 ms, total: 25.4 ms\n",
      "Wall time: 358 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[11] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/*.csv'\n",
    "noaa_csv_path = DATASET_FOLDER + 'year_2016-01'\n",
    "inventory_path = DATASET_FOLDER + 'ghcnd-inventory.txt'\n",
    "sc.stop()\n",
    "total = 0\n",
    "# configuration\n",
    "APP_NAME = 'Filter on Rain'\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(SPARK_MASTER)\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Load inventory\n",
    "inventory_rdd = sc.textFile(inventory_path)\n",
    "# Filter on Rain\n",
    "rain_inventory_rdd = inventory_rdd.filter(lambda s : \"PRCP\" in s)\n",
    "# Now we have a list of all files containings precipitation data\n",
    "# Keep only the first column\n",
    "# Format each RDD as (K, V) to prepare for the join operation\n",
    "rain_inventory_rdd = rain_inventory_rdd.map(lambda line : (line[0:11], line[36:])) # Keep code and years\n",
    "#rain_inventory_rdd = rain_inventory_rdd.filter(lambda s: s[0:11])\n",
    "#rain_inventory = rain_inventory_rdd.collect()\n",
    "# Load the stations data points\n",
    "print(f'Processing {noaa_csv_path}...')\n",
    "all_data_rdd = sc.textFile(noaa_csv_path+'/*')\n",
    "all_data_rdd = all_data_rdd.map(lambda line: (line[1:12], line[14:]))\n",
    "# Keep only precipitation data\n",
    "join = rain_inventory_rdd.join(all_data_rdd)\n",
    "#rain_rdd  = files_rdd.filter(lambda s : s[1:12] in rain_inventory)\n",
    "# Save precipitation data\n",
    "output = DATASET_FOLDER + 'year_2016-01_rain'\n",
    "print(f'Saving to {output}')\n",
    "join_output = join.map(lambda x: ','.join([x[0],x[1][1]]))\n",
    "#sc.parallelize(join_output.take(2)).collect()\n",
    "# Flatten the result\n",
    "#join = rdd.map(lambda x: ','.join([x[0],x[1][0],x[1][1]]))\n",
    "# Get all partition on one node, to have one file (don't do it for huge dataset)\n",
    "join_output = join_output.repartition(1)\n",
    "\n",
    "shutil.rmtree(output, ignore_errors=True)\n",
    "join_output.saveAsTextFile(output)\n",
    "total = join.count()\n",
    "sc.stop()\n",
    "\n",
    "total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/56957589/how-to-read-multiple-csv-files-with-different-schema-in-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"PRCP\",\"PRCP_ATTRIBUTES\",\"SNWD\",\"SNWD_ATTRIBUTES\",\"TMAX\",\"TMAX_ATTRIBUTES\",\"TMIN\",\"TMIN_ATTRIBUTES\",\"TAVG\",\"TAVG_ATTRIBUTES\"\n",
      "FRM00007005,\"2016-01-01\",\"50.143\",\"1.832\",\"67.1\",\"ABBEVILLE, FR\",\"    5\",\",,S\",,,\"   88\",\",,S\",\"   21\",\",,S\",\"   56\",\"H,,S\"\n",
      "FRM00007180,\"2016-01-01\",\"48.692\",\"6.23\",\"228.9\",\"ESSEY, FR\",\"   15\",\",,S\",,,\"   60\",\",,S\",\"    9\",\",,S\",\"   44\",\"H,,S\"\n",
      "FRM00007535,\"2016-01-01\",\"44.745\",\"1.3967\",\"260.0\",\"GOURDON, FR\",\"   12\",\",,E\",,,\"  137\",\",,E\",\"    9\",\",,E\",\"   84\",\"H,,S\"\n",
      "FRM00007607,\"2016-01-01\",\"43.912\",\"-0.508\",\"61.9\",\"MONT DE MARSAN, FR\",\"    5\",\",,S\",,,,,\"    9\",\",,S\",\"   77\",\"H,,S\"\n",
      "FRM00007240,\"2016-01-01\",\"47.432\",\"0.728\",\"108.8\",\"VAL DE LOIRE, FR\",\"    0\",\",,S\",,,\"   96\",\",,S\",,,\"   68\",\"H,,S\"\n",
      "FRM00007790,\"2016-01-01\",\"42.553\",\"9.484\",\"7.9\",\"PORETTA, FR\",\"    0\",\",,S\",,,\"  133\",\",,S\",,,\"  107\",\"H,,S\"\n",
      "FRM00007558,\"2016-01-01\",\"44.117\",\"3.017\",\"720.0\",\"MILLAU, FR\",\"   48\",\",,S\",,,,,\"   37\",\",,S\",\"   66\",\"H,,S\"\n",
      "FRM00007621,\"2016-01-01\",\"43.179\",\"-0.006\",\"384.0\",\"LOURDES, FR\",\"    5\",\",,S\",,,\"  141\",\",,S\",,,\"   76\",\"H,,S\"\n",
      "FRM00007335,\"2016-01-01\",\"46.588\",\"0.307\",\"128.9\",\"BIARD, FR\",\"    3\",\",,S\",,,,,\"   45\",\",,S\",\"   77\",\"H,,S\"\n",
      "FRM00061998,\"2016-01-01\",\"-49.35\",\"70.25\",\"30.0\",\"PORT AUX FRANCAIS ILES KERGU, FR\",\"    3\",\",,S\",,,\"  126\",\",,S\",\"   38\",\",,S\",\"   76\",\"H,,S\"\n"
     ]
    }
   ],
   "source": [
    "!echo '\"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"PRCP\",\"PRCP_ATTRIBUTES\",\"SNWD\",\"SNWD_ATTRIBUTES\",\"TMAX\",\"TMAX_ATTRIBUTES\",\"TMIN\",\"TMIN_ATTRIBUTES\",\"TAVG\",\"TAVG_ATTRIBUTES\"'\n",
    "!head $DATASET_FOLDER/year_2016-01_rain/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('FRM00007005',\n",
       "  ('1973 2020',\n",
       "   '\"2016-01-01\",\"50.143\",\"1.832\",\"67.1\",\"ABBEVILLE, FR\",\"    5\",\",,S\",,,\"   88\",\",,S\",\"   21\",\",,S\",\"   56\",\"H,,S\"')),\n",
       " ('FRM00007180',\n",
       "  ('1973 2020',\n",
       "   '\"2016-01-01\",\"48.692\",\"6.23\",\"228.9\",\"ESSEY, FR\",\"   15\",\",,S\",,,\"   60\",\",,S\",\"    9\",\",,S\",\"   44\",\"H,,S\"'))]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(join.take(2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRM00007005,\"2016-01-01\",\"50.143\",\"1.832\",\"67.1\",\"ABBEVILLE, FR\",\"    5\",\",,S\",,,\"   88\",\",,S\",\"   21\",\",,S\",\"   56\",\"H,,S\"',\n",
       " 'FRM00007180,\"2016-01-01\",\"48.692\",\"6.23\",\"228.9\",\"ESSEY, FR\",\"   15\",\",,S\",,,\"   60\",\",,S\",\"    9\",\",,S\",\"   44\",\"H,,S\"']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"ORLY\" ID : 0-20000-0-07149\n",
    "\n",
    "Coordinates: 48.7166666667°N, 2.3844444444°E, 89m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd $DATASET_FOLDER/year_2016-01_rain && (ls | xargs cat)  > ../year_2016-01_rain.csv\n",
    "# ! head $DATASET_FOLDER/year_2016-01_rain.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noaa_csv_path = '/media/data-nvme/dev/datasets/WorldBank//noaa/ASN00060066.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/AE*.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/*.csv'\n",
    "noaa_csv_path = DATASET_FOLDER + 'year_2016-01.csv'\n",
    "def extract_rain():\n",
    "    total = 0\n",
    "    # configuration\n",
    "    APP_NAME = 'Count Rain'\n",
    "    conf = SparkConf().setAppName(APP_NAME)\n",
    "    conf = conf.setMaster(SPARK_MASTER)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    try:\n",
    "        # core part of the script\n",
    "        print(f'Processing {noaa_csv_path}...')\n",
    "        files_rdd = sc.textFile(noaa_csv_path+'/*')\n",
    "        total = files_rdd.count()\n",
    "    except Exception as inst:\n",
    "        print('ERROR')\n",
    "        print(type(inst))    # the exception instance\n",
    "        print(inst.args)     # arguments stored in .args\n",
    "        print(inst)\n",
    "        raise\n",
    "    finally:\n",
    "        sc.stop()\n",
    "        return total\n",
    "\n",
    "total = extract_rain()\n",
    "#print(f'{totalLength:%.2f}')\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $DATASET_FOLDER/year_2016.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "customSchema = StructType([ \\\n",
    "StructField(\"asset_id\", StringType(), True), \\\n",
    "StructField(\"price_date\", StringType(), True), \\\n",
    "etc., \n",
    "StructField(\"close_price\", StringType(), True), \\\n",
    "StructField(\"filename\", StringType(), True)])\n",
    "\n",
    "\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "   .option(\"header\", \"false\") \\\n",
    "   .option(\"sep\",\"|\") \\\n",
    "   .schema(customSchema) \\\n",
    "   .load(fullPath) \\\n",
    "   .withColumn(\"filename\", input_file_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"USR0000AFRA\",\"2016-01-25\",\"35.8456\",\"-113.055\",\"2063.5\",\"FRAZIER WELLS ARIZONA, AZ US\",\"   39\",\"H,,U\",\"  -39\",\"H,,U\",\"  -10\",\",,U\"',\n",
       " '\"FRM00007149\",\"2016-01-01\",\"48.7167\",\"2.3842\",\"89.0\",\"ORLY, FR\",\"   10\",\",,E\",,,\"   85\",\",,E\",\"   38\",\",,E\",\"   62\",\"H,,S\"']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('test')\n",
    "conf = conf.setMaster(SPARK_MASTER)\n",
    "sc = SparkContext(conf=conf)\n",
    "s = ['\"USR0000AFRA\",\"2016-01-25\",\"35.8456\",\"-113.055\",\"2063.5\",\"FRAZIER WELLS ARIZONA, AZ US\",\"   39\",\"H,,U\",\"  -39\",\"H,,U\",\"  -10\",\",,U\"']\n",
    "s += ['\"FRM00007149\",\"2016-01-01\",\"48.7167\",\"2.3842\",\"89.0\",\"ORLY, FR\",\"   10\",\",,E\",,,\"   85\",\",,E\",\"   38\",\",,E\",\"   62\",\"H,,S\"']\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('USR0000AFR', ('2016-01-25', '35.8456')),\n",
       " ('FRM0000714', ('2016-01-01', '48.7167'))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(s)\n",
    "#test = rain_inventor_rdd.map(lambda line : (line[0:11], line[36:])) # Keep code and years\n",
    "rdd = rdd.map(lambda line : (line[1:11], (line[15:25], line[28:35])))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('USR0000AFR', ('2016-01-25', '35.8456')),\n",
       " ('FRM0000714', ('2016-01-01', '48.7167'))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USR0000AFR,2016-01-25,35.8456', 'FRM0000714,2016-01-01,48.7167']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: ','.join([x[0],x[1][0],x[1][1]])).repartition(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
